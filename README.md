# MNIST_Assignment3
This assignment showcase the effect of drop out and sequence of batch size. 
Using Dropout Normalization technique and building various architecture
By using Dropout layer tried various architecture for getting desired accuracy on test data.
Depending upon the ***combination and sequence of batch size*** gives different result. 
From this assignment it is observed that adding dropout layer at hidden layers with *high drop out percentage will deteriorate the accuracy* and it may tends to overfitting the model.
But with appropriate drop out percentage it will reduce gap between training accuracy and validation accuracy.
To achieve best result than this there is need to perform numerous trials.
